# AI notes

> 范围：
>
> 人工智能 {机器学习 {监督学习，无监督学习，强化学习}}
>
> p.s. 监督学习（分类、回归）和无监督学习（聚合、降维）含有：深度学习
>
> p.s. 强化学习含有：深度强化学习

#### 1. 监督学习

分类学习：分类问题中，输出是离散的，可以说是离散变量预测，定性输出称为分类

回归学习：回归问题中，输出是连续的，可以说是连续变量预测，定量输出称为回归



##### 1.1 分类学习

###### 1.1.1 朴素贝叶斯 (线性)

(Navie Bayes)

- 朴素贝叶斯属于生成模型

- 朴素贝叶斯存在条件独立性假设，即对于模型中的每个条件来说，他们发生的概率都是相互独立的，因此主要缺点是，不能学习特征间的相互作用

- 当某个特征出现的概率为0时，将会出现某个类别的概率为0，这是不符合概率规律的，因此朴素贝叶斯需要加入smoothing平滑，分别有Epsilon平滑和Laplace平滑
- Epsilon平滑：计算时将所有为0的概率附上一个极小的值
- Laplace平滑：引入一个alpha系数，在计算分母时 +M*alpha，M为该特征的类型数量；在计算分子时+alpha
- 朴素贝叶斯的优点是训练的速度较快，仅仅只是特征概率的数学运算；对缺失数据不太敏感，通常用于文本分类；朴素贝叶斯对结果解释容易理解
- 缺点是对输入数据的表达形式很敏感；且当样本特征有关联时，分类效果不好



###### 1.1.2 K近邻 (非线性)

(K-Nearest Neighbor)



###### 1.1.3 决策树 (非线性)

(Decision Tree)

- 决策树可以有效的处理特征间的相互关系，因此对异常值不敏感
- 决策树很重要的一点是选择一个属性进行分枝，因此要使用信息增益和信息熵来判断选择哪一个属性
- Entropy信息熵：即是意外或不确定性的预期水平
- Information gain信息增益：对数据进行分枝之前和之后的熵减少的量
- 即，如果单个类的概率很高，则信息熵较低，该事件是可预测的；如果概率在多个类中间平均分配，则信息熵较高，事件是不可预测的。偏向于选择信息熵较低的事件
- IG = H(R) - mean-info(R_A) 因此，信息熵越低，信息增益越高
- 优点：比较适合处理有缺失属性的样本；可以同时处理字符型和数值型样本
- 缺点：容易发生过拟合



###### 1.1.4 随机森林 (非线性)

(Random Forest)

- 随机森林是利用了多颗决策树的机器学习算法，随机森林会随机创造决策树来组成森林，以此将各个决策树的输出整合起来生成最后的输出结果
- 随机森林能解决决策树过拟合的问题，决策树+bagging即是随机森林
- bagging：随机抽取原始数据作为各个样本，基于各个样本独立进行模型预测，最后整合结果；bagging是有放回抽样



###### 1.1.5 支持向量机 (线性)

(SVM)



##### 1.2 回归学习

###### 1.2.1 逻辑回归 (线性)

(Logistic Regression)



###### 1.2.2 支持向量回归 (线性)



###### 1.2.3 K近邻 (非线性)



#### 2. 无监督学习

##### 2.1 K-means

pass



##### 2.2 Clustering

pass



#### 3. 深度学习 (非线性)

##### 3.1 多层感知机

(Multi-layer Perceptron)

p.s. 感知机Perceptron是线性模型



##### 3.2 卷积神经网络

(CNN)

- 卷积：神经网络某一时刻的输出是由多个输入共同作用叠加的结果
- 卷子层中存在数据表格，称为卷积核
- 原始图片经过输入层后变为灰度或是RGB数值填充的矩阵，然后再与卷积核进行运算得到新矩阵，新矩阵叫特征图
- 池化层：能选取图像的主要特征，常用的Max-pooling是保留窗口覆盖区域的最大数值，矩阵被池化后，参数会大量减少
- 全连接层：通常在CNN网络的最后，将提取到的特征集合在一起，给出图片可能是某种事物的概率



##### 3.3 循环神经网络

(RNN)

- 循环神经网络高度重视序列顺序信息，因为前后的输入和当前的信息都是有关系的，因此循环神经网络赋予了每个输入权重矩阵，在计算下一个隐藏层的时候，每个输入都会乘上相对应的权重系数进行计算
- 其次，循环神经网络使用状态向量来表示先前处理过的上下文
- 两个simple RNN可以结合成双向循环神经网络，每个时刻的输入都会同时提供给两个方向相反的RNN，而最后的输出则由这两个RNN共同决定
- 循环神经网络使用反向传播法计算梯度，因此当网络越来越深的时候，梯度会越来越小，最后出现梯度消失的情况



##### 3.4 长短时记忆网络

(LSTM)

- RNN虽然有一定的记忆能力，但只能保留短期记忆，并且会发生梯度消失的情况，因此提出门这个概念，LSTM在RNN的基础上增加了输入门，遗忘门，和输出门，用来决定信息如何保留
- 遗忘门：用来决定需要遗忘当前记忆单元memory cell内的多少信息
- 输入门：决定了当前网络信息有多少要被保存到记忆单元内
- 输出门：决定了多大程度的输出当前记忆单元的信息来生成下一个状态
- LSTM有多层LSTM和双向LSTM的应用



##### 3.5 负反馈神经网络

pass



##### 3.6 Transformer神经网络

- 可以处理n to n的数据分析，即seq2seq，一个拥有编码器和解码器的模型
- 先由编码器提取原始句子的意义，再由解码器将意义转换成对应的语言
- 为了解决意义单元无法有效处理长句子的问题，提出attention注意力机制，不断从原始句子中提取生成对应单词时最需要的信息，成功解决输入序列长度的限制，但是因为这样计算效率很低，因此使用改进后的self-attention自注意力机制，即Transformer神经网络的原理
- 自注意力机制：编码器先提取所有单词的意义，解码器再依据生成顺序选取所需要的信息，不会再回头处理原句子，这样的结构支持并行计算，效率更高
- Transformer + BERT：BERT预训练模型将编码器提取所有单词的意义部分独立出来形成一个预训练模型，能较准确的用向量将每个词语表示出来





































