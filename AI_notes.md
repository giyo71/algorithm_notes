# AI notes

> 范围：
>
> 人工智能 {机器学习 {监督学习，无监督学习，强化学习}}
>
> p.s. 监督学习（分类、回归）和无监督学习（聚合、降维）含有：深度学习
>
> p.s. 强化学习含有：深度强化学习

#### 1. 监督学习

分类学习：分类问题中，输出是离散的，可以说是离散变量预测，定性输出称为分类

回归学习：回归问题中，输出是连续的，可以说是连续变量预测，定量输出称为回归



##### 1.1 分类学习

###### 1.1.1 朴素贝叶斯 (线性)

(Navie Bayes)

- 朴素贝叶斯属于生成模型

- 朴素贝叶斯存在条件独立性假设，即对于模型中的每个条件来说，他们发生的概率都是相互独立的，因此主要缺点是，不能学习特征间的相互作用

- 当某个特征出现的概率为0时，将会出现某个类别的概率为0，这是不符合概率规律的，因此朴素贝叶斯需要加入smoothing平滑，分别有Epsilon平滑和Laplace平滑
- Epsilon平滑：计算时将所有为0的概率附上一个极小的值
- Laplace平滑：引入一个alpha系数，在计算分母时 +M*alpha，M为该特征的类型数量；在计算分子时+alpha
- 朴素贝叶斯的优点是训练的速度较快，仅仅只是特征概率的数学运算；对缺失数据不太敏感，通常用于文本分类；朴素贝叶斯对结果解释容易理解
- 缺点是对输入数据的表达形式很敏感；且当样本特征有关联时，分类效果不好



###### 1.1.2 K近邻 (非线性)

(K-Nearest Neighbor)



###### 1.1.3 决策树 (非线性)

(Decision Tree)

- 决策树可以有效的处理特征间的相互关系，因此对异常值不敏感
- 决策树很重要的一点是选择一个属性进行分枝，因此要使用信息增益和信息熵来判断选择哪一个属性
- Entropy信息熵：即是意外或不确定性的预期水平
- Information gain信息增益：对数据进行分枝之前和之后的熵减少的量
- 即，如果单个类的概率很高，则信息熵较低，该事件是可预测的；如果概率在多个类中间平均分配，则信息熵较高，事件是不可预测的。偏向于选择信息熵较低的事件
- IG = H(R) - mean-info(R_A) 因此，信息熵越低，信息增益越高
- 优点：比较适合处理右缺失属性的样本；可以同时处理字符型和熟知型样本



###### 1.1.4 随机森林 (非线性)

(Random Forest)



###### 1.1.5 支持向量机 (线性)

(SVM)



##### 1.2 回归学习

###### 1.2.1 逻辑回归 (线性)

(Logistic Regression)



###### 1.2.2 支持向量回归 (线性)



###### 1.2.3 K近邻 (非线性)



#### 2. 无监督学习

##### 2.1 K-means

pass



##### 2.2 Clustering

pass



#### 3. 深度学习 (非线性)

##### 3.1 多层感知机

(Multi-layer Perceptron)

p.s. 感知机Perceptron是线性模型



##### 3.2 卷积神经网络

pass



##### 3.3 循环神经网络

pass



##### 3.4 长短时记忆网络

pass



##### 3.5 负反馈神经网络

pass



##### 3.6 Traansformer神经网络

pass



##### 3.7 Bert预训练模型

pass



